{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open field behavior analysis\n",
    "Determines centroid (center-of-mass) of animal in open field arena videos, then analyzes centroid displacement and plots calculated metrics. Performs Gaussian Mix Modeling to determine thresholds for lingering vs progressing movement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process videos\n",
    "\n",
    "\"\"\"\n",
    "Processes open-field videos by:\n",
    "1) Loading and validating each video,\n",
    "2) Thresholding and refining the largest contour to isolate the animal’s body,\n",
    "3) Computing geometric properties (area, perimeter, aspect ratio, solidity, orientation),\n",
    "4) Saving per-frame metrics (e.g., centroid coordinates) to a CSV,\n",
    "5) Creating a side-by-side masked video with a highlighted centroid.\n",
    "\n",
    "Expects video filenames of the form: 'AA###_AA##' (e.g. 'CK314_PD11-trim.mp4').\n",
    "\n",
    "Args:\n",
    "    folder_path (str): Path to the directory containing the video files.\n",
    "    threshold_value (int or float): Threshold used for segmenting the subject.f\n",
    "        Higher values make the mask more selective; lower values make it more inclusive.\n",
    "\n",
    "Outputs:\n",
    "    - For each valid video, a CSV file capturing frame-by-frame metrics (centroid, area, perimeter, etc.).\n",
    "    - A corresponding MP4 video with the original frames side-by-side with the segmented mask.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "import os\n",
    "import gc\n",
    "\n",
    "TIMESTAMP = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "class OpenFieldVideoProcessor:\n",
    "    \"\"\"\n",
    "    Loads a video file, extracts metadata, and provides processing methods.\n",
    "    Expects filenames in the format: AA###_AA## (e.g. CK314_PD11-trim.mp4).\n",
    "    \"\"\"\n",
    "    def __init__(self, video_path: str):\n",
    "        self.file_path = Path(video_path)\n",
    "        self.animal_id = self.extract_animal_id()\n",
    "        self.day_id = self.extract_day_id()\n",
    "        \n",
    "        cap = cv2.VideoCapture(str(self.file_path))\n",
    "        if not cap.isOpened():\n",
    "            raise ValueError(f\"Cannot open video file: {self.file_path}\")\n",
    "        self.frame_rate = cap.get(cv2.CAP_PROP_FPS)\n",
    "        self.num_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        self.shape = (\n",
    "            int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)),\n",
    "            int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        )\n",
    "        cap.release()\n",
    "\n",
    "    def load_video(self) -> cv2.VideoCapture:\n",
    "        cap = cv2.VideoCapture(str(self.file_path))\n",
    "        if not cap.isOpened():\n",
    "            raise ValueError(f\"Cannot open video file: {self.file_path}\")\n",
    "        return cap\n",
    "\n",
    "    def extract_animal_id(self) -> str:\n",
    "        pattern = re.compile(r'^[A-Za-z]{2}\\d{3}')\n",
    "        match = pattern.search(self.file_path.stem)\n",
    "        if match:\n",
    "            return match.group(0)\n",
    "        raise ValueError(f\"File name {self.file_path.name} does not match expected animal ID format.\")\n",
    "\n",
    "    def extract_day_id(self) -> str:\n",
    "        pattern = re.compile(r'_[A-Za-z]{2}\\d{2}')\n",
    "        match = pattern.search(self.file_path.stem)\n",
    "        if match:\n",
    "            return match.group(0)[1:]\n",
    "        raise ValueError(f\"File name {self.file_path.name} does not match expected day ID format.\")\n",
    "\n",
    "def create_output_directory(input_folder, base_name=\"analysis\"):\n",
    "    parent_dir = Path(input_folder).resolve().parent\n",
    "    output_dir = parent_dir / f\"{base_name}_{TIMESTAMP}\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    return output_dir\n",
    "\n",
    "def process_video(video_path, threshold_value):\n",
    "    \"\"\"\n",
    "    Processes the video and returns a dict with keys:\n",
    "    'frames', 'masks', 'centroids', 'mask_area', 'mask_perimeter',\n",
    "    'mask_aspect_ratio', 'mask_solidity', 'mask_orientation'\n",
    "    \"\"\"\n",
    "    video_obj = OpenFieldVideoProcessor(video_path)\n",
    "    cap = video_obj.load_video()\n",
    "    frames, masks, centroids = [], [], []\n",
    "    mask_area_list = []\n",
    "    mask_perimeter_list = []\n",
    "    mask_aspect_ratio_list = []\n",
    "    mask_solidity_list = []\n",
    "    mask_orientation_list = []\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frames.append(frame)\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        blurred = cv2.GaussianBlur(gray, (11, 11), 0)  # Default is 5, 5\n",
    "        _, mask = cv2.threshold(blurred, threshold_value, 255, cv2.THRESH_BINARY_INV)\n",
    "        kernel = np.ones((3, 3), np.uint8)\n",
    "        mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        if contours:\n",
    "            # Fill the largest contour\n",
    "            largest = max(contours, key=cv2.contourArea)\n",
    "            refined = np.zeros_like(mask)\n",
    "            cv2.drawContours(refined, [largest], -1, 255, thickness=cv2.FILLED)\n",
    "            \n",
    "            # Apply distance transform to remove the thin tail\n",
    "            dist = cv2.distanceTransform(refined, cv2.DIST_L2, 5)\n",
    "            max_val = dist.max()\n",
    "            \n",
    "            # After applying distance transform and thresholding\n",
    "            _, body_mask = cv2.threshold(dist, 0.1 * max_val, 255, cv2.THRESH_BINARY)\n",
    "            body_mask = np.uint8(body_mask)\n",
    "\n",
    "            # Find contours in the new body mask\n",
    "            body_contours, _ = cv2.findContours(body_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "            # Create a new mask with only the largest body contour\n",
    "            if body_contours:\n",
    "                largest_body = max(body_contours, key=cv2.contourArea)\n",
    "                final_mask = np.zeros_like(body_mask)\n",
    "                cv2.drawContours(final_mask, [largest_body], -1, 255, thickness=cv2.FILLED)\n",
    "                mask = final_mask\n",
    "            else:\n",
    "                mask = body_mask  # Fallback if no contours found\n",
    "\n",
    "            # Recompute metrics using contours from the new mask\n",
    "            new_contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "            if new_contours:\n",
    "                largest_body = max(new_contours, key=cv2.contourArea)\n",
    "                area = cv2.contourArea(largest_body)\n",
    "                perimeter = cv2.arcLength(largest_body, True)\n",
    "                x_rect, y_rect, w_rect, h_rect = cv2.boundingRect(largest_body)\n",
    "                aspect_ratio = float(w_rect) / h_rect if h_rect > 0 else np.nan\n",
    "                hull = cv2.convexHull(largest_body)\n",
    "                hull_area = cv2.contourArea(hull)\n",
    "                solidity = area / hull_area if hull_area > 0 else np.nan\n",
    "                if len(largest_body) >= 5:\n",
    "                    ellipse = cv2.fitEllipse(largest_body)\n",
    "                    orientation = ellipse[2]\n",
    "                else:\n",
    "                    orientation = np.nan\n",
    "            else:\n",
    "                area = cv2.contourArea(largest)\n",
    "                perimeter = cv2.arcLength(largest, True)\n",
    "                x_rect, y_rect, w_rect, h_rect = cv2.boundingRect(largest)\n",
    "                aspect_ratio = float(w_rect) / h_rect if h_rect > 0 else np.nan\n",
    "                hull = cv2.convexHull(largest)\n",
    "                hull_area = cv2.contourArea(hull)\n",
    "                solidity = area / hull_area if hull_area > 0 else np.nan\n",
    "                if len(largest) >= 5:\n",
    "                    ellipse = cv2.fitEllipse(largest)\n",
    "                    orientation = ellipse[2]\n",
    "                else:\n",
    "                    orientation = np.nan\n",
    "        else:\n",
    "            area = np.nan\n",
    "            perimeter = np.nan\n",
    "            aspect_ratio = np.nan\n",
    "            solidity = np.nan\n",
    "            orientation = np.nan\n",
    "\n",
    "        mask_area_list.append(area)\n",
    "        mask_perimeter_list.append(perimeter)\n",
    "        mask_aspect_ratio_list.append(aspect_ratio)\n",
    "        mask_solidity_list.append(solidity)\n",
    "        mask_orientation_list.append(orientation)\n",
    "\n",
    "        # Compute centroid using moments on the final mask\n",
    "        M = cv2.moments(mask)\n",
    "        if M['m00'] != 0:\n",
    "            cx = M['m10'] / M['m00']\n",
    "            cy = M['m01'] / M['m00']\n",
    "            centroids.append((cx, cy))\n",
    "        else:\n",
    "            centroids.append(None)\n",
    "\n",
    "        masks.append(mask)\n",
    "\n",
    "    cap.release()\n",
    "    return {\n",
    "        \"frames\": frames,\n",
    "        \"masks\": masks,\n",
    "        \"centroids\": centroids,\n",
    "        \"mask_area\": mask_area_list,\n",
    "        \"mask_perimeter\": mask_perimeter_list,\n",
    "        \"mask_aspect_ratio\": mask_aspect_ratio_list,\n",
    "        \"mask_solidity\": mask_solidity_list,\n",
    "        \"mask_orientation\": mask_orientation_list\n",
    "    }, video_obj\n",
    "\n",
    "def create_masked_video(data, output_path, start_frame=0, end_frame=None, fps=30):\n",
    "    \"\"\"\n",
    "    Creates a side-by-side masked video showing original frames and their binary masks with centroid overlay.\n",
    "    \"\"\"\n",
    "    if not data:\n",
    "        print(\"No data provided.\")\n",
    "        return\n",
    "\n",
    "    frames = data['frames']\n",
    "    masks = data['masks']\n",
    "    centroids = data['centroids']\n",
    "    \n",
    "    total_frames = len(frames)\n",
    "    if end_frame is None or end_frame > total_frames:\n",
    "        end_frame = total_frames\n",
    "\n",
    "    if start_frame < 0 or start_frame >= total_frames or start_frame >= end_frame:\n",
    "        print(\"Invalid frame range.\")\n",
    "        return\n",
    "\n",
    "    height, width, _ = frames[0].shape\n",
    "    composite_width = width * 2\n",
    "    \n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(str(output_path), fourcc, fps, (composite_width, height))\n",
    "    \n",
    "    for frame, mask, centroid in zip(frames[start_frame:end_frame], masks[start_frame:end_frame], centroids[start_frame:end_frame]):\n",
    "        frame_copy = frame.copy()\n",
    "        mask_bgr = cv2.cvtColor(mask, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "        if centroid is not None:\n",
    "            cv2.circle(frame_copy, (int(centroid[0]), int(centroid[1])), 10, (0, 0, 255), -1)  # cv.2circle expects integer coordinates\n",
    "            cv2.circle(mask_bgr, (int(centroid[0]), int(centroid[1])), 10, (0, 0, 255), -1)\n",
    "        \n",
    "        composite = np.hstack((frame_copy, mask_bgr))\n",
    "        out.write(composite)\n",
    "    \n",
    "    out.release()\n",
    "    print(f\"Masked video saved to: {output_path}\")\n",
    "\n",
    "def main(folder_path, threshold_value):\n",
    "    folder = Path(folder_path)\n",
    "    output_dir = create_output_directory(folder, base_name=\"openfield_analysis_mask\")\n",
    "    masked_videos_dir = output_dir / \"masked_videos\"\n",
    "    os.makedirs(masked_videos_dir, exist_ok=True)\n",
    "    \n",
    "    video_pattern = re.compile(r'^[A-Za-z]{2}\\d{3}_[A-Za-z]{2}\\d{2}')\n",
    "    \n",
    "    for video_file in folder.iterdir():\n",
    "        if video_file.is_file() and video_file.suffix.lower() in ['.mp4', '.avi'] and video_pattern.search(video_file.stem):\n",
    "            try:\n",
    "                data, video_obj = process_video(str(video_file), threshold_value)\n",
    "            except ValueError as e:\n",
    "                print(e)\n",
    "                continue\n",
    "            \n",
    "            frames = list(range(len(data[\"centroids\"])))\n",
    "            real_times = [f / video_obj.frame_rate for f in frames]\n",
    "            \n",
    "            df = pd.DataFrame({\n",
    "                \"frame\": frames,\n",
    "                \"real_time_s\": real_times,\n",
    "                \"centroid_x\": [c[0] if c is not None else np.nan for c in data[\"centroids\"]],\n",
    "                \"centroid_y\": [c[1] if c is not None else np.nan for c in data[\"centroids\"]],\n",
    "                \"mask_area\": data[\"mask_area\"],\n",
    "                \"mask_perimeter\": data[\"mask_perimeter\"],\n",
    "                \"mask_aspect_ratio\": data[\"mask_aspect_ratio\"],\n",
    "                \"mask_solidity\": data[\"mask_solidity\"],\n",
    "                \"mask_orientation\": data[\"mask_orientation\"],\n",
    "                \"animal_id\": video_obj.animal_id,\n",
    "                \"day_id\": video_obj.day_id\n",
    "            })\n",
    "            csv_filename = f\"{video_obj.animal_id}_{video_obj.day_id}.csv\"\n",
    "            df.to_csv(output_dir / csv_filename, index=False)\n",
    "            print(f\"{csv_filename} saved to {output_dir}\")\n",
    "            \n",
    "            masked_video_path = masked_videos_dir / f\"{video_obj.animal_id}_{video_obj.day_id}_masked.mp4\"\n",
    "            create_masked_video(\n",
    "                data=data,\n",
    "                output_path=masked_video_path,\n",
    "                start_frame=0,\n",
    "                end_frame=None,\n",
    "                fps=int(video_obj.frame_rate)\n",
    "            )\n",
    "            \n",
    "            del data, df\n",
    "            gc.collect()\n",
    "    \n",
    "    print(\"Per-frame CSVs and masked videos saved in:\", output_dir)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    folder_path = r\"D:\\CK3_open_field\\videos\"  # Directory containing open field videos\n",
    "    main(folder_path, threshold_value=75)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the centroid data and compute summary metrics\n",
    "\n",
    "\"\"\"\n",
    "Reads centroid tracking CSVs from open-field experiments, computes:\n",
    "- Displacement, velocity, acceleration, angle of movement\n",
    "- Mean velocity, mean acceleration, total distance\n",
    "- Percent-of-baseline distance (if day ID == \"PD00\" is available)\n",
    "- Conversion to centimeters using PX_PER_CM\n",
    "\n",
    "It saves a processed CSV for each original file with \"_processed.csv\" appended,\n",
    "and aggregates all summaries into \"summary_metrics.csv\".\n",
    "\n",
    "Args:\n",
    "    analysis_folder (str): Path to the folder containing centroid CSV files.\n",
    "\n",
    "Outputs:\n",
    "    - Per-file processed CSV files: Each original CSV gains columns for displacement,\n",
    "      velocity, acceleration, angle, and is saved with a \"_processed.csv\" suffix.\n",
    "    - A summary_metrics.csv containing aggregate statistics for each file, including\n",
    "      total distance, mean velocity, mean acceleration, and percent-of-baseline\n",
    "      distances (both in pixels and centimeters).\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def compute_summary_metrics(df):\n",
    "    times = df[\"real_time_s\"].values\n",
    "    dt = np.median(np.diff(times)) if len(times) > 1 else 1.0\n",
    "    frame_rate = 1 / dt if dt != 0 else 1.0\n",
    "\n",
    "    # Displacement from original centroid\n",
    "    displacement = [np.nan]\n",
    "    for i in range(1, len(df)):\n",
    "        x0, y0 = df.loc[i - 1, [\"centroid_x\", \"centroid_y\"]]\n",
    "        x1, y1 = df.loc[i, [\"centroid_x\", \"centroid_y\"]]\n",
    "        if pd.isna(x0) or pd.isna(y0) or pd.isna(x1) or pd.isna(y1):\n",
    "            displacement.append(np.nan)\n",
    "        else:\n",
    "            dx = x1 - x0\n",
    "            dy = y1 - y0\n",
    "            displacement.append(np.sqrt(dx**2 + dy**2))\n",
    "    displacement = pd.Series(displacement).interpolate().tolist()\n",
    "\n",
    "    velocity = [np.nan] + [displacement[i] / dt if not np.isnan(displacement[i]) else np.nan \n",
    "                            for i in range(1, len(displacement))]\n",
    "\n",
    "    acceleration = [np.nan]\n",
    "    for i in range(1, len(velocity)):\n",
    "        if np.isnan(velocity[i]) or np.isnan(velocity[i - 1]):\n",
    "            acceleration.append(np.nan)\n",
    "        else:\n",
    "            acceleration.append((velocity[i] - velocity[i - 1]) / dt)\n",
    "\n",
    "    angles = [np.nan]\n",
    "    for i in range(1, len(df)):\n",
    "        x0, y0 = df.loc[i - 1, [\"centroid_x\", \"centroid_y\"]]\n",
    "        x1, y1 = df.loc[i, [\"centroid_x\", \"centroid_y\"]]\n",
    "        if pd.isna(x0) or pd.isna(y0) or pd.isna(x1) or pd.isna(y1):\n",
    "            angles.append(np.nan)\n",
    "        else:\n",
    "            dx = x1 - x0\n",
    "            dy = y1 - y0\n",
    "            angles.append(np.degrees(np.arctan2(dy, dx)))\n",
    "\n",
    "    df[\"displacement_px\"] = displacement\n",
    "    df[\"velocity_px_s\"] = velocity\n",
    "    df[\"acceleration_px_s2\"] = acceleration\n",
    "    df[\"angle_deg\"] = angles\n",
    "\n",
    "    total_distance = np.nansum(displacement)\n",
    "    mean_velocity = np.nanmean(velocity)\n",
    "    mean_acceleration = np.nanmean(acceleration)\n",
    "\n",
    "    return {\n",
    "        \"total_distance_px\": total_distance,\n",
    "        \"mean_velocity_px_s\": mean_velocity,\n",
    "        \"mean_acceleration_px_s2\": mean_acceleration,\n",
    "        \"frame_rate\": frame_rate\n",
    "    }, df\n",
    "\n",
    "def main(analysis_folder):\n",
    "    folder = Path(analysis_folder)\n",
    "    summary_list = []\n",
    "    pattern = re.compile(r\"^[A-Za-z]{2}\\d{3}_PD\\d{2}\\.csv$\")\n",
    "\n",
    "    csv_files = [f for f in folder.iterdir() if f.is_file() and pattern.match(f.name)]\n",
    "    for csv_file in csv_files:\n",
    "        df = pd.read_csv(csv_file)\n",
    "        metrics, df_processed = compute_summary_metrics(df)\n",
    "        summary_list.append({\n",
    "            \"animal_id\": df_processed[\"animal_id\"].iloc[0],\n",
    "            \"day_id\": df_processed[\"day_id\"].iloc[0],\n",
    "            \"total_distance_px\": metrics[\"total_distance_px\"],\n",
    "            \"mean_velocity_px_s\": metrics[\"mean_velocity_px_s\"],\n",
    "            \"mean_acceleration_px_s2\": metrics[\"mean_acceleration_px_s2\"]\n",
    "        })\n",
    "\n",
    "        processed_path = folder / (csv_file.stem + \"_processed.csv\")\n",
    "        df_processed.to_csv(processed_path, index=False)\n",
    "\n",
    "    if summary_list:\n",
    "        summary_df = pd.DataFrame(summary_list)\n",
    "        # Baseline computed from PD00 for pixels.\n",
    "        baseline_dict = summary_df[summary_df[\"day_id\"] == \"PD00\"].set_index(\"animal_id\")[\"total_distance_px\"].to_dict()\n",
    "        summary_df[\"percent_baseline_distance\"] = summary_df.apply(\n",
    "            lambda row: (row[\"total_distance_px\"] / baseline_dict[row[\"animal_id\"]] * 100)\n",
    "            if row[\"animal_id\"] in baseline_dict and baseline_dict[row[\"animal_id\"]] != 0 else np.nan,\n",
    "            axis=1\n",
    "        )\n",
    "        # Compute total distance in cm.\n",
    "        summary_df[\"total_distance_cm\"] = summary_df[\"total_distance_px\"] / PX_PER_CM\n",
    "        \n",
    "        # Compute baseline in cm.\n",
    "        baseline_dict_cm = {animal: distance_px / PX_PER_CM for animal, distance_px in baseline_dict.items()}\n",
    "        summary_df[\"total_distance_cm_pct_baseline\"] = summary_df.apply(\n",
    "            lambda row: (row[\"total_distance_cm\"] / baseline_dict_cm[row[\"animal_id\"]] * 100)\n",
    "            if row[\"animal_id\"] in baseline_dict_cm and baseline_dict_cm[row[\"animal_id\"]] != 0 else np.nan,\n",
    "            axis=1\n",
    "        )\n",
    "\n",
    "        summary_df.to_csv(folder / \"summary_metrics.csv\", index=False)\n",
    "\n",
    "    print(\"Processing complete. Processed CSVs and summary saved in:\", folder)\n",
    "\n",
    "\n",
    "PX_PER_CM = 16.8144\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_folder = r\"D:\\CK3_open_field\\openfield_analysis_mask_2025-03-31\"\n",
    "    main(input_folder)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting functions for the processed CSV files\n",
    "\n",
    "\"\"\"\n",
    "Generates visualizations for processed open-field tracking data, including:\n",
    "1. Trajectory plots with a user-specified arena rectangle (bounding box),\n",
    "2. Velocity over time (raw vs. smoothed, in px/s and cm/s),\n",
    "3. Per-frame metrics plots (mask area, perimeter, etc., with px→cm conversions),\n",
    "4. Bar charts and line plots summarizing total distance, velocity, and acceleration,\n",
    "   along with % of baseline metrics when available.\n",
    "\n",
    "Flow:\n",
    "    - Reads any \"*_processed.csv\" files in `analysis_folder`.\n",
    "    - Creates an \"analysis_plots\" folder for storing individual plots (e.g., trajectories, velocity, etc.).\n",
    "    - If \"summary_metrics.csv\" is present, generates additional summary plots and baseline comparisons.\n",
    "\n",
    "Args:\n",
    "    analysis_folder (str): Directory containing processed per-frame CSVs and an optional \"summary_metrics.csv\".\n",
    "\n",
    "Outputs:\n",
    "    - Trajectory PNGs for each file with valid centroid data (includes a bounding box defined by `arena_rect`),\n",
    "    - Velocity plots (px/s, cm/s, raw vs. smoothed),\n",
    "    - Line plots for mask metrics (area, perimeter, aspect ratio, solidity, orientation),\n",
    "    - Summary bar/line plots comparing total distance, velocity, acceleration, and baseline percentages across days/animals.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import os\n",
    "from matplotlib.patches import Rectangle\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "\n",
    "\n",
    "def plot_velocity_comparison(df, output_dir):\n",
    "    if \"velocity_px_s\" not in df.columns or \"velocity_px_s_smooth\" not in df.columns:\n",
    "        return\n",
    "    plt.figure(figsize=(12, 6), dpi=300)\n",
    "    plt.plot(df[\"real_time_s\"], df[\"velocity_px_s\"], linestyle='-', label='Raw Velocity (px/s)')\n",
    "    plt.plot(df[\"real_time_s\"], df[\"velocity_px_s_smooth\"], linestyle='-', label='Smoothed Velocity (px/s)')\n",
    "    plt.title(f\"Velocity Comparison: {df.animal_id.iloc[0]} {df.day_id.iloc[0]}\")\n",
    "    plt.xlabel(\"Time (s)\")\n",
    "    plt.ylabel(\"Velocity (px/s)\")\n",
    "    plt.legend(loc='best')\n",
    "    out_path = output_dir / f\"{df.animal_id.iloc[0]}_{df.day_id.iloc[0]}_velocity_comparison.png\"\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path, dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "def plot_velocity_comparison_cm(df, output_dir):\n",
    "    if \"velocity_px_s\" not in df.columns or \"velocity_px_s_smooth\" not in df.columns:\n",
    "        return\n",
    "    plt.figure(figsize=(12, 6), dpi=300)\n",
    "    plt.plot(df[\"real_time_s\"], df[\"velocity_px_s\"] / PX_PER_CM, linestyle='-', label='Raw Velocity (cm/s)')\n",
    "    plt.plot(df[\"real_time_s\"], df[\"velocity_px_s_smooth\"] / PX_PER_CM, linestyle='-', label='Smoothed Velocity (cm/s)')\n",
    "    plt.title(f\"Velocity Comparison (cm/s): {df.animal_id.iloc[0]} {df.day_id.iloc[0]}\")\n",
    "    plt.xlabel(\"Time (s)\")\n",
    "    plt.ylabel(\"Velocity (cm/s)\")\n",
    "    plt.legend(loc='best')\n",
    "    out_path = output_dir / f\"{df.animal_id.iloc[0]}_{df.day_id.iloc[0]}_velocity_comparison_cm.png\"\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path, dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "def plot_trajectory(df, output_dir, arena_rect=(25, 25, 650, 650)):\n",
    "    valid = df.dropna(subset=[\"centroid_x\", \"centroid_y\"])\n",
    "    x = valid[\"centroid_x\"].values\n",
    "    y = valid[\"centroid_y\"].values\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(6, 6), dpi=600)\n",
    "    ax.add_patch(Rectangle((arena_rect[0], arena_rect[1]),\n",
    "                           arena_rect[2], arena_rect[3],\n",
    "                           edgecolor='black', facecolor='none', linewidth=2))\n",
    "    ax.plot(x, y, color='#ff7f0e', linewidth=1)  # Original blue color: #1E88E5, Original orange color: #ff7f0e, New blue: #9AAACE, New orange: #EAA973\n",
    "    ax.scatter(x[0], y[0], color='black', s=40, zorder=3)\n",
    "    ax.set_xlim(arena_rect[0], arena_rect[0] + arena_rect[2])\n",
    "    ax.set_ylim(arena_rect[1], arena_rect[1] + arena_rect[3])\n",
    "    ax.invert_yaxis()\n",
    "    ax.set_aspect('equal')\n",
    "    ax.axis('off')\n",
    "\n",
    "    animal_id = str(df.animal_id.iloc[0])\n",
    "    day_id = str(df.day_id.iloc[0])\n",
    "    out_path = output_dir / f\"{animal_id}_{day_id}_trajectory.png\"\n",
    "    plt.savefig(out_path, dpi=600, bbox_inches='tight', pad_inches=0.05)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_velocity(df, output_dir):\n",
    "    if \"velocity_px_s\" not in df.columns:\n",
    "        return\n",
    "    plt.figure(figsize=(12, 6), dpi=300)\n",
    "    plt.plot(df[\"real_time_s\"], df[\"velocity_px_s\"], marker='o', linestyle='-')\n",
    "    plt.title(f\"Velocity Over Time: {df.animal_id.iloc[0]} {df.day_id.iloc[0]}\")\n",
    "    plt.xlabel(\"Time (s)\")\n",
    "    plt.ylabel(\"Velocity (px/s)\")\n",
    "    out_path = output_dir / f\"{df.animal_id.iloc[0]}_{df.day_id.iloc[0]}_velocity_px_s.png\"\n",
    "    plt.savefig(out_path, dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "def plot_velocity_cm(df, output_dir):\n",
    "    if \"velocity_px_s\" not in df.columns:\n",
    "        return\n",
    "    plt.figure(figsize=(12, 6), dpi=300)\n",
    "    plt.plot(df[\"real_time_s\"], df[\"velocity_px_s\"] / PX_PER_CM, marker='o', linestyle='-')\n",
    "    plt.title(f\"Velocity Over Time (cm/s): {df.animal_id.iloc[0]} {df.day_id.iloc[0]}\")\n",
    "    plt.xlabel(\"Time (s)\")\n",
    "    plt.ylabel(\"Velocity (cm/s)\")\n",
    "    out_path = output_dir / f\"{df.animal_id.iloc[0]}_{df.day_id.iloc[0]}_velocity_cm_s.png\"\n",
    "    plt.savefig(out_path, dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "def plot_line_parameter(df, parameter, title, ylabel, output_dir, animal_id, day_id):\n",
    "    plt.figure(figsize=(12, 6), dpi=300)\n",
    "    plt.plot(df[\"real_time_s\"], df[parameter], marker='o', linestyle='-')\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Time (s)\")\n",
    "    plt.ylabel(ylabel)\n",
    "    out_path = output_dir / f\"{animal_id}_{day_id}_{parameter}.png\"\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path, dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "def plot_summary_metrics(summary_df, output_dir):\n",
    "    summary_df[\"id\"] = summary_df[\"animal_id\"] + \"_\" + summary_df[\"day_id\"]\n",
    "    metrics = [\n",
    "        (\"total_distance_px\", \"Total Distance (px)\"),\n",
    "        (\"mean_velocity_px_s\", \"Mean Velocity (px/s)\"),\n",
    "        (\"mean_acceleration_px_s2\", \"Mean Acceleration (px/s²)\")\n",
    "    ]\n",
    "    for col, title in metrics:\n",
    "        plt.figure(figsize=(12, 6), dpi=300)\n",
    "        plt.bar(summary_df[\"id\"], summary_df[col])\n",
    "        plt.title(title)\n",
    "        plt.ylabel(title)\n",
    "        plt.xlabel(\"Video ID\")\n",
    "        plt.xticks(rotation=45, ha=\"right\")\n",
    "        out_path = output_dir / f\"summary_{col}.png\"\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(out_path, dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "def plot_summary_metrics_cm(summary_df, output_dir):\n",
    "    summary_df[\"id\"] = summary_df[\"animal_id\"] + \"_\" + summary_df[\"day_id\"]\n",
    "    metrics = [\n",
    "        (\"total_distance_cm\", \"Total Distance (cm)\"),\n",
    "        (\"mean_velocity_cm_s\", \"Mean Velocity (cm/s)\"),\n",
    "        (\"mean_acceleration_cm_s2\", \"Mean Acceleration (cm/s²)\")\n",
    "    ]\n",
    "    for col, title in metrics:\n",
    "        plt.figure(figsize=(12, 6), dpi=300)\n",
    "        plt.bar(summary_df[\"id\"], summary_df[col])\n",
    "        plt.title(title)\n",
    "        plt.ylabel(title)\n",
    "        plt.xlabel(\"Video ID\")\n",
    "        plt.xticks(rotation=45, ha=\"right\")\n",
    "        out_path = output_dir / f\"summary_{col}.png\"\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(out_path, dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "def plot_percent_baseline(summary_df, output_dir):\n",
    "    plt.figure(figsize=(12, 6), dpi=300)\n",
    "    for animal_id, group in summary_df.groupby(\"animal_id\"):\n",
    "        sorted_group = group.sort_values(\"day_id\")\n",
    "        plt.plot(sorted_group[\"day_id\"], sorted_group[\"percent_baseline_distance\"],\n",
    "                 marker='o', label=animal_id)\n",
    "    plt.axhline(100, color='gray', linestyle='--', linewidth=1)\n",
    "    plt.title(\"Total Distance as % of Baseline (PD00)\")\n",
    "    plt.ylabel(\"Percent of Baseline Distance (%)\")\n",
    "    plt.xlabel(\"Day ID\")\n",
    "    plt.legend(title=\"Animal ID\", loc='best')\n",
    "    plt.tight_layout()\n",
    "    out_path = output_dir / \"percent_baseline_total_distance.png\"\n",
    "    plt.savefig(out_path, dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "def plot_raw_total_distance(summary_df, output_dir):\n",
    "    plt.figure(figsize=(12, 6), dpi=300)\n",
    "    for animal_id, group in summary_df.groupby(\"animal_id\"):\n",
    "        sorted_group = group.sort_values(\"day_id\")\n",
    "        plt.plot(sorted_group[\"day_id\"], sorted_group[\"total_distance_px\"],\n",
    "                 marker='o', label=animal_id)\n",
    "    plt.title(\"Raw Total Distance over Days\")\n",
    "    plt.ylabel(\"Total Distance (px)\")\n",
    "    plt.xlabel(\"Day ID\")\n",
    "    plt.legend(title=\"Animal ID\", loc='best')\n",
    "    plt.tight_layout()\n",
    "    out_path = output_dir / \"raw_total_distance.png\"\n",
    "    plt.savefig(out_path, dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "def plot_raw_total_distance_cm(summary_df, output_dir):\n",
    "    plt.figure(figsize=(12, 6), dpi=300)\n",
    "    for animal_id, group in summary_df.groupby(\"animal_id\"):\n",
    "        sorted_group = group.sort_values(\"day_id\")\n",
    "        plt.plot(sorted_group[\"day_id\"], sorted_group[\"total_distance_cm\"],\n",
    "                 marker='o', label=animal_id)\n",
    "    plt.title(\"Raw Total Distance over Days (cm)\")\n",
    "    plt.ylabel(\"Total Distance (cm)\")\n",
    "    plt.xlabel(\"Day ID\")\n",
    "    plt.legend(title=\"Animal ID\", loc='best')\n",
    "    plt.tight_layout()\n",
    "    out_path = output_dir / \"raw_total_distance_cm.png\"\n",
    "    plt.savefig(out_path, dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "def main(analysis_folder):\n",
    "    folder = Path(analysis_folder)\n",
    "    plots_dir = folder / \"analysis_plots\"\n",
    "    os.makedirs(plots_dir, exist_ok=True)\n",
    "    \n",
    "    processed_files = list(folder.glob(\"**/*_processed.csv\"))\n",
    "    for proc_file in processed_files:\n",
    "        df = pd.read_csv(proc_file)\n",
    "        animal_id = df[\"animal_id\"].iloc[0]\n",
    "        day_id = df[\"day_id\"].iloc[0]\n",
    "        animal_folder = plots_dir / animal_id\n",
    "        os.makedirs(animal_folder, exist_ok=True)\n",
    "        \n",
    "        plot_trajectory(df, animal_folder)\n",
    "        plot_velocity(df, animal_folder)\n",
    "        plot_velocity_cm(df, animal_folder)\n",
    "        plot_velocity_comparison(df, animal_folder)\n",
    "        plot_velocity_comparison_cm(df, animal_folder)\n",
    "        \n",
    "        parameters = [\n",
    "            (\"mask_area\", \"Mask Area Over Time\", \"Area_px2\"),\n",
    "            (\"mask_perimeter\", \"Mask Perimeter Over Time\", \"Perimeter_px\"),\n",
    "            (\"mask_aspect_ratio\", \"Mask Aspect Ratio Over Time\", \"Aspect_Ratio\"),\n",
    "            (\"mask_solidity\", \"Mask Solidity Over Time\", \"Solidity\"),\n",
    "            (\"mask_orientation\", \"Mask Orientation Over Time\", \"Orientation_deg\")\n",
    "        ]\n",
    "        for param, title, ylabel in parameters:\n",
    "            if param in df.columns:\n",
    "                plot_line_parameter(df, param, title, ylabel, animal_folder, animal_id, day_id)\n",
    "                if param == \"mask_area\":\n",
    "                    # Convert area from px^2 to cm^2.\n",
    "                    new_param = param + \"_cm2\"\n",
    "                    df[new_param] = df[param] / (PX_PER_CM ** 2)\n",
    "                    new_title = title.replace(\"Area\", \"Area (cm²)\")\n",
    "                    plot_line_parameter(df, new_param, new_title, \"Area (cm²)\", animal_folder, animal_id, day_id)\n",
    "                elif param == \"mask_perimeter\":\n",
    "                    new_param = param + \"_cm\"\n",
    "                    df[new_param] = df[param] / PX_PER_CM\n",
    "                    new_title = title.replace(\"Perimeter\", \"Perimeter (cm)\")\n",
    "                    plot_line_parameter(df, new_param, new_title, \"Perimeter (cm)\", animal_folder, animal_id, day_id)\n",
    "    \n",
    "    summary_path = folder / \"summary_metrics.csv\"\n",
    "    if summary_path.exists():\n",
    "        summary_df = pd.read_csv(summary_path)\n",
    "        # Add cm conversions for summary metrics.\n",
    "        summary_df[\"total_distance_cm\"] = summary_df[\"total_distance_px\"] / PX_PER_CM\n",
    "        summary_df[\"mean_velocity_cm_s\"] = summary_df[\"mean_velocity_px_s\"] / PX_PER_CM\n",
    "        summary_df[\"mean_acceleration_cm_s2\"] = summary_df[\"mean_acceleration_px_s2\"] / PX_PER_CM\n",
    "        \n",
    "        plot_summary_metrics(summary_df, plots_dir)\n",
    "        plot_summary_metrics_cm(summary_df, plots_dir)\n",
    "        plot_percent_baseline(summary_df, plots_dir)\n",
    "        plot_raw_total_distance(summary_df, plots_dir)\n",
    "        plot_raw_total_distance_cm(summary_df, plots_dir)\n",
    "\n",
    "    print(\"Plotting complete. Plots saved in:\", plots_dir)\n",
    "\n",
    "\n",
    "INPUT_FOLDER = r\"D:\\CK3_open_field\\openfield_analysis_mask_2025-03-31\\temp_recolor\"\n",
    "PX_PER_CM = 16.8144\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(analysis_folder=INPUT_FOLDER)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze time spent in the middle of an arena\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "def process_file(file, arena_width, arena_height, output_dir, middle_pct):\n",
    "    \"\"\"\n",
    "    Read CSV with 'centroid_x','centroid_y'; compute:\n",
    "      - in_middle: inside central square (middle_pct of arena)\n",
    "      - cross_into_middle: entry events (outside→inside), first frame excluded\n",
    "      - in_middle_post_cross: in_middle after ≥1 cross_into_middle event\n",
    "    Returns augmented df and summary metrics.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file)\n",
    "    df.columns = df.columns.str.strip()\n",
    "\n",
    "    # define center bounds\n",
    "    m = middle_pct / 100\n",
    "    x_min, x_max = arena_width * (1 - m) / 2, arena_width * (1 + m) / 2\n",
    "    y_min, y_max = arena_height * (1 - m) / 2, arena_height * (1 + m) / 2\n",
    "\n",
    "    # boolean flags\n",
    "    df['in_middle'] = (\n",
    "        df['centroid_x'].between(x_min, x_max) &\n",
    "        df['centroid_y'].between(y_min, y_max)\n",
    "    )\n",
    "    df['cross_into_middle'] = df['in_middle'] & ~df['in_middle'].shift(fill_value=False)\n",
    "    df.loc[df.index[0], 'cross_into_middle'] = False\n",
    "\n",
    "    # post‐cross flag: true from first entry onward (includes crossing frame)\n",
    "    df['in_middle_post_cross'] = df['in_middle'] & (df['cross_into_middle'].cumsum() >= 1)\n",
    "\n",
    "    # save processed CSV\n",
    "    out_csv = output_dir / f\"{file.stem}_middle.csv\"\n",
    "    df.to_csv(out_csv, index=False)\n",
    "\n",
    "    # summary metrics\n",
    "    pct_time = df['in_middle'].mean() * 100\n",
    "    pct_time_post = df['in_middle_post_cross'].mean() * 100\n",
    "    n_cross = df['cross_into_middle'].sum()\n",
    "\n",
    "    # extract IDs\n",
    "    parts = file.stem.split('_')\n",
    "    animal_id = parts[0] if len(parts) >= 2 else \"unknown\"\n",
    "    day_id = parts[1] if len(parts) >= 2 else \"unknown\"\n",
    "\n",
    "    # annotate df for plotting if needed\n",
    "    df['animal_id'], df['day_id'] = animal_id, day_id\n",
    "\n",
    "    return df, animal_id, day_id, pct_time, pct_time_post, n_cross\n",
    "\n",
    "def plot_trajectory(df, output_dir, arena_width, arena_height, animal_id, day_id, middle_pct):\n",
    "    \"\"\"\n",
    "    Plot trajectory with arena border and dashed center square.\n",
    "    \"\"\"\n",
    "    valid = df.dropna(subset=[\"centroid_x\",\"centroid_y\"])\n",
    "    x, y = valid[\"centroid_x\"], valid[\"centroid_y\"]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(6,6), dpi=600)\n",
    "    # arena border (25px margin)\n",
    "    ax.add_patch(Rectangle((25,25), arena_width-50, arena_height-50,\n",
    "                           edgecolor='black', facecolor='none', linewidth=2))\n",
    "    # center square\n",
    "    m = middle_pct/100\n",
    "    cx, cy = arena_width*(1-m)/2, arena_height*(1-m)/2\n",
    "    ax.add_patch(Rectangle((cx,cy), arena_width*m, arena_height*m,\n",
    "                           edgecolor='black', facecolor='none', linestyle='--', linewidth=2))\n",
    "    ax.plot(x, y, linewidth=1)\n",
    "    ax.scatter(x.iat[0], y.iat[0], color='black', s=40, zorder=3)\n",
    "    ax.set_xlim(25, arena_width-25)\n",
    "    ax.set_ylim(25, arena_height-25)\n",
    "    ax.invert_yaxis()\n",
    "    ax.set_aspect('equal')\n",
    "    ax.axis('off')\n",
    "    plt.savefig(output_dir / f\"{animal_id}_{day_id}_trajectory.png\", dpi=600,\n",
    "                bbox_inches='tight', pad_inches=0.05)\n",
    "    plt.close(fig)\n",
    "\n",
    "def plot_metric_line(summary_df, metric, ylabel, title, out_path):\n",
    "    \"\"\"\n",
    "    Line plot of metric vs. days post-stroke, per animal.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10,6))\n",
    "    for ani, grp in summary_df.groupby('animal_id'):\n",
    "        g = grp.copy()\n",
    "        g['day_num'] = g['day_id'].str.extract(r'PD(\\d{2})').astype(int)\n",
    "        g = g.sort_values('day_num')\n",
    "        plt.plot(g['day_num'], g[metric], marker='o', label=ani)\n",
    "    if metric.startswith('norm_'):\n",
    "        plt.axhline(100, linestyle='--', color='grey')\n",
    "    # x‐axis labels\n",
    "    days = summary_df[['day_id']].drop_duplicates()\n",
    "    days['day_num'] = days['day_id'].str.extract(r'PD(\\d{2})').astype(int)\n",
    "    days = days.sort_values('day_num')\n",
    "    plt.xticks(days['day_num'], days['day_id'])\n",
    "    plt.xlabel(\"Days Post Stroke\")\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path, dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "def process_all_files(analysis_folder, arena_width, arena_height, middle_pct):\n",
    "    folder = Path(analysis_folder)\n",
    "    out_root = folder / \"time_spent_middle_analysis\"\n",
    "    out_root.mkdir(exist_ok=True, parents=True)\n",
    "    plots_dir = out_root / \"individual_plots\"\n",
    "    plots_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    pattern = re.compile(r\"^[A-Za-z]+\\d+_PD\\d{2}\\.csv$\")\n",
    "    files = [f for f in folder.iterdir() if f.is_file() and pattern.match(f.name)]\n",
    "\n",
    "    summary = []\n",
    "    for file in files:\n",
    "        df, ani, day, pct, pct_post, n_cross = process_file(\n",
    "            file, arena_width, arena_height, out_root, middle_pct\n",
    "        )\n",
    "        summary.append({\n",
    "            'file': file.name,\n",
    "            'animal_id': ani,\n",
    "            'day_id': day,\n",
    "            'pct_time_in_middle': pct,\n",
    "            'pct_time_in_mid_post_cross': pct_post,\n",
    "            'n_crossings': n_cross\n",
    "        })\n",
    "        plot_trajectory(df, plots_dir, arena_width, arena_height, ani, day, middle_pct)\n",
    "\n",
    "    return pd.DataFrame(summary), out_root\n",
    "\n",
    "def main(analysis_folder, arena_width=700, arena_height=700, middle_pct=50):\n",
    "    summary_df, out_root = process_all_files(\n",
    "        analysis_folder, arena_width, arena_height, middle_pct\n",
    "    )\n",
    "    # normalize metrics to PD00 per animal\n",
    "    for col in ['pct_time_in_middle', 'pct_time_in_mid_post_cross', 'n_crossings']:\n",
    "        norm_col = f\"norm_{col}\"\n",
    "        summary_df[norm_col] = np.nan\n",
    "        for ani, grp in summary_df.groupby('animal_id'):\n",
    "            base = grp[grp['day_id']=='PD00']\n",
    "            if not base.empty and base.iloc[0][col]:\n",
    "                factor = 100 / base.iloc[0][col]\n",
    "                idx = summary_df['animal_id']==ani\n",
    "                summary_df.loc[idx, norm_col] = summary_df.loc[idx, col] * factor\n",
    "\n",
    "    # save summary\n",
    "    summary_df.to_csv(out_root / \"middle_analysis_summary.csv\", index=False)\n",
    "\n",
    "    # existing plots\n",
    "    plot_metric_line(summary_df, 'pct_time_in_middle',\n",
    "                     'Percentage Time in Middle',\n",
    "                     'Time Spent in Middle per Animal',\n",
    "                     out_root / \"raw_pct_time_in_middle.png\")\n",
    "    plot_metric_line(summary_df, 'n_crossings',\n",
    "                     'Number of Crossings',\n",
    "                     'Crossings into Middle per Animal',\n",
    "                     out_root / \"raw_n_crossings.png\")\n",
    "    plot_metric_line(summary_df, 'norm_pct_time_in_middle',\n",
    "                     'Normalized % Time in Middle',\n",
    "                     'Normalized Time in Middle per Animal',\n",
    "                     out_root / \"norm_pct_time_in_middle.png\")\n",
    "    plot_metric_line(summary_df, 'norm_n_crossings',\n",
    "                     'Normalized Number of Crossings',\n",
    "                     'Normalized Crossings into Middle per Animal',\n",
    "                     out_root / \"norm_n_crossings.png\")\n",
    "    plot_metric_line(summary_df, 'pct_time_in_mid_post_cross',\n",
    "                     '% Time Post‐Cross in Middle',\n",
    "                     'Time in Middle After First Re‐entry',\n",
    "                     out_root / \"raw_pct_time_post_cross.png\")\n",
    "    plot_metric_line(summary_df, 'norm_pct_time_in_mid_post_cross',\n",
    "                     'Normalized % Post‐Cross Time in Middle',\n",
    "                     'Normalized Post‐Cross Time in Middle',\n",
    "                     out_root / \"norm_pct_time_post_cross.png\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ANALYSIS_FOLDER = r\"D:\\CK3_open_field\\openfield_analysis_mask_2025-03-31\\total_distance_analysis\"\n",
    "    main(ANALYSIS_FOLDER)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Time Spent Moving Analysis\n",
    "\n",
    "Automates Gaussian Mixture Model (GMM) analysis on baseline (PD00) files to establish\n",
    "a velocity threshold distinguishing 'moving' vs. 'stationary' frames, then applies\n",
    "that threshold to subsequent files, with optional removal of jump timepoints.\n",
    "\n",
    "Args:\n",
    "    analysis_folder (str): Directory with '*_PD##.csv' files containing centroid data.\n",
    "    smoothing_n (int): Rolling window size for velocity smoothing (default 1 = no smoothing).\n",
    "\n",
    "Outputs:\n",
    "    - 'gmm_analysis_auto' directory:\n",
    "      1. 'individual_plots': Histograms for each PD00 file showing the GMM fit,\n",
    "      2. 'time_spent_moving': Per-day classification CSVs with 'is_moving' and 'is_removed_jump' flags,\n",
    "         plus a summary of time spent moving (raw and normalized), velocity while moving,\n",
    "         jump-removal-adjusted velocity metrics, and line plots.\n",
    "      3. Plots of BIC scores, velocity PDFs, and line plots showing time spent moving both with and without jumps.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from pathlib import Path\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import brentq\n",
    "\n",
    "# ------------------------------\n",
    "# Global Variables & Jump Loading\n",
    "# ------------------------------\n",
    "ANALYSIS_FOLDER = r\"D:\\CK3_open_field\\openfield_analysis_mask_2025-03-31\\time_spent_moving_analysis\\temp_velocity\"\n",
    "SMOOTHING_N     = 1       # Simple Moving Average. 1 = no smoothing\n",
    "PX_PER_CM       = 16.8144 # Conversion factor from pixels to cm.\n",
    "JUMP_FILE_PATH  = r\"D:\\CK3_open_field\\openfield_analysis_mask_2025-03-31\\jumping_timestamps_2025-05-08.csv\"  # Set to None to disable\n",
    "\n",
    "# load jump ranges {(animal, day): [(start,end), ...]}\n",
    "jumps = {}\n",
    "if JUMP_FILE_PATH:\n",
    "    jf = pd.read_csv(JUMP_FILE_PATH)\n",
    "    jf.columns = jf.columns.str.strip()\n",
    "    for _, row in jf.dropna(subset=['frame_start','frame_end']).iterrows():\n",
    "        key = (row['Animal'], row['Day'])\n",
    "        jumps.setdefault(key, []).append((int(row['frame_start']), int(row['frame_end'])))\n",
    "\n",
    "# ------------------------------\n",
    "# GMM Utility Functions\n",
    "# ------------------------------\n",
    "def find_intersection(means, covars, weights):\n",
    "    m0, m1 = means\n",
    "    s0, s1 = np.sqrt(covars)\n",
    "    w0, w1 = weights\n",
    "    def diff(x):\n",
    "        return w0 * norm.pdf(x, m0, s0) - w1 * norm.pdf(x, m1, s1)\n",
    "    lower = max(0, min(m0 - 4*s0, m1 - 4*s1))\n",
    "    upper = max(m0 + 4*s0, m1 + 4*s1)\n",
    "    xs = np.linspace(lower, upper, 1000)\n",
    "    diffs = diff(xs)\n",
    "    changes = np.where(np.diff(np.sign(diffs)))[0]\n",
    "    if not changes.size:\n",
    "        return np.nan\n",
    "    idx = changes[0]\n",
    "    return brentq(diff, xs[idx], xs[idx+1])\n",
    "\n",
    "def find_n_components_bic(data, max_components=5):\n",
    "    bics, models = [], []\n",
    "    for n in range(1, max_components+1):\n",
    "        gmm = GaussianMixture(n_components=n, random_state=0).fit(data)\n",
    "        bics.append(gmm.bic(data))\n",
    "        models.append(gmm)\n",
    "    x = np.arange(1, max_components+1)\n",
    "    y = np.array(bics)\n",
    "    vec = np.array([x[-1]-x[0], y[-1]-y[0]])\n",
    "    norm_vec = vec/np.linalg.norm(vec)\n",
    "    offsets = np.vstack((x-x[0], y-y[0])).T\n",
    "    proj = np.outer(np.dot(offsets, norm_vec), norm_vec)\n",
    "    dist = np.linalg.norm(offsets-proj, axis=1)\n",
    "    best_idx = np.argmax(dist)\n",
    "    return best_idx+1, models[best_idx], bics\n",
    "\n",
    "def compute_velocity(df, smoothing_n):\n",
    "    dx = np.diff(df['centroid_x'])\n",
    "    dy = np.diff(df['centroid_y'])\n",
    "    dt = np.diff(df['real_time_s'])\n",
    "    velocity = np.sqrt(dx**2 + dy**2)/dt\n",
    "    df['velocity_px_s'] = np.nan\n",
    "    df.loc[1:, 'velocity_px_s'] = velocity\n",
    "    df['smoothed_velocity_px_s'] = df['velocity_px_s'].rolling(window=smoothing_n, min_periods=1).mean()\n",
    "    return df\n",
    "\n",
    "# ------------------------------\n",
    "# Plotting Functions\n",
    "# ------------------------------\n",
    "def plot_individual_histogram(res, plots_dir, smoothing_n):\n",
    "    aid = res['base_name'].split('_')[0]\n",
    "    valid = res['valid_velocities']\n",
    "    gmm = res['gmm']; thr = res['threshold']; nc = res['n_components']\n",
    "    vr = np.linspace(valid.min(), valid.max(), 1000).reshape(-1,1)\n",
    "    pdf_tot = np.exp(gmm.score_samples(vr))\n",
    "    w, m, c = gmm.weights_, gmm.means_.flatten(), gmm.covariances_.flatten()\n",
    "    fig, ax = plt.subplots(figsize=(8,5))\n",
    "    ax.hist(valid, bins=100, density=True, alpha=0.5)\n",
    "    ax.plot(vr, pdf_tot, color='black')\n",
    "    for i in range(nc):\n",
    "        ax.plot(vr, w[i]*norm.pdf(vr.flatten(), m[i], np.sqrt(c[i])), linestyle='--')\n",
    "    ax.axvline(thr, color='red', linestyle=':')\n",
    "    ax.set_title(f'{aid} (gmm-{nc}, smooth-{smoothing_n})')\n",
    "    ax.set_xlabel('Smoothed Velocity (px/s)'); ax.set_ylabel('Density')\n",
    "    ax.legend([f'Comp {i}' for i in range(nc)] + [f'Thr {thr:.2f}'])\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(plots_dir / f\"{res['base_name']}_gmm-{nc}_smooth-{smoothing_n}_hist.png\", dpi=300)\n",
    "    plt.close(fig)\n",
    "\n",
    "def plot_bic_scores(pd00_res, out_root, smoothing_n, global_nc):\n",
    "    fig, ax = plt.subplots(figsize=(8,6))\n",
    "    for res in pd00_res.values():\n",
    "        bics, nc = res['bics'], res['n_components']\n",
    "        x = np.arange(1, len(bics)+1)\n",
    "        ax.plot(x, bics, label=f\"{res['base_name']} (best={nc})\")\n",
    "        ax.scatter(nc, bics[nc-1], color='black')\n",
    "    ax.set_xticks(np.arange(1,6))\n",
    "    ax.set_xlabel('Components'); ax.set_ylabel('BIC')\n",
    "    ax.set_title(f'BIC Scores (gmm-{global_nc}, smooth-{smoothing_n})'); ax.legend()\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(out_root / f\"gmm-{global_nc}_smooth-{smoothing_n}_bic_scores_pd00.png\", dpi=300)\n",
    "    plt.close(fig)\n",
    "\n",
    "def plot_distribution_pdf(pd00_res, out_root, smoothing_n, global_nc):\n",
    "    fig, ax = plt.subplots(figsize=(10,6))\n",
    "    for res in pd00_res.values():\n",
    "        valid, gmm, thr = res['valid_velocities'], res['gmm'], res['threshold']\n",
    "        vr = np.linspace(valid.min(), valid.max(), 1000).reshape(-1,1)\n",
    "        ax.plot(vr, np.exp(gmm.score_samples(vr)), label=res['base_name'])\n",
    "        ax.axvline(thr, linestyle='--', color='grey')\n",
    "    ax.set_xlabel('Smoothed Velocity (px/s)'); ax.set_ylabel('Density')\n",
    "    ax.set_title(f'GMM PDFs (gmm-{global_nc}, smooth-{smoothing_n})'); ax.legend()\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(out_root / f\"gmm-{global_nc}_smooth-{smoothing_n}_velocity_pdfs_pd00.png\", dpi=300)\n",
    "    plt.close(fig)\n",
    "\n",
    "def plot_time_spent_line(df, folder, smoothing_n, global_nc):\n",
    "    plt.figure(figsize=(10,6))\n",
    "    for aid, grp in df.groupby('animal_id'):\n",
    "        g = grp.copy(); g['dnum'] = g['day_id'].str.extract(r'PD(\\d{2})').astype(int)\n",
    "        g = g.sort_values('dnum')\n",
    "        plt.plot(g['dnum'], g['pct_time_moving'], marker='o', label=aid)\n",
    "    days = df[['day_id']].drop_duplicates(); days['dnum'] = days['day_id'].str.extract(r'PD(\\d{2})').astype(int)\n",
    "    days = days.sort_values('dnum')\n",
    "    plt.xticks(days['dnum'], days['day_id']); plt.ylim(0, None)\n",
    "    plt.xlabel('Days Post Stroke'); plt.ylabel('% Time Moving')\n",
    "    plt.title(f'Time Moving (gmm-{global_nc}, smooth-{smoothing_n})'); plt.legend(); plt.tight_layout()\n",
    "    plt.savefig(folder / f\"gmm-{global_nc}_smooth-{smoothing_n}_time_spent_line.png\", dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "def plot_normalized_time_spent_line(df, folder, smoothing_n, global_nc):\n",
    "    plt.figure(figsize=(10,6))\n",
    "    for aid, grp in df.groupby('animal_id'):\n",
    "        g = grp.copy(); g['dnum'] = g['day_id'].str.extract(r'PD(\\d{2})').astype(int)\n",
    "        g = g.sort_values('dnum')\n",
    "        plt.plot(g['dnum'], g['pct_time_moving_pct-baseline'], marker='o', label=aid)\n",
    "    plt.axhline(0, linestyle='--', color='grey')  # baseline at 0% change\n",
    "    days = df[['day_id']].drop_duplicates(); days['dnum'] = days['day_id'].str.extract(r'PD(\\d{2})').astype(int)\n",
    "    days = days.sort_values('dnum')\n",
    "    plt.xticks(days['dnum'], days['day_id']); plt.ylim(0, None)\n",
    "    plt.xlabel('Days Post Stroke'); plt.ylabel('% Change Time Moving')\n",
    "    plt.title(f'Normalized Time Moving (gmm-{global_nc}, smooth-{smoothing_n})'); plt.legend(); plt.tight_layout()\n",
    "    plt.savefig(folder / f\"gmm-{global_nc}_smooth-{smoothing_n}_time_spent_norm_line.png\", dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "def plot_velocity_while_moving_line(df, folder, smoothing_n, global_nc):\n",
    "    plt.figure(figsize=(10,6))\n",
    "    for aid, grp in df.groupby('animal_id'):\n",
    "        g = grp.copy(); g['dnum'] = g['day_id'].str.extract(r'PD(\\d{2})').astype(int)\n",
    "        g = g.sort_values('dnum')\n",
    "        plt.plot(g['dnum'], g['mean_velocity_while_moving_cm_s'], marker='o', label=aid)\n",
    "    days = df[['day_id']].drop_duplicates(); days['dnum'] = days['day_id'].str.extract(r'PD(\\d{2})').astype(int)\n",
    "    days = days.sort_values('dnum')\n",
    "    plt.xticks(days['dnum'], days['day_id']); plt.ylim(0, None)\n",
    "    plt.xlabel('Days Post Stroke'); plt.ylabel('Mean Vel (cm/s)')\n",
    "    plt.title(f'Velocity Moving (gmm-{global_nc}, smooth-{smoothing_n})'); plt.legend(); plt.tight_layout()\n",
    "    plt.savefig(folder / f\"gmm-{global_nc}_smooth-{smoothing_n}_vel_moving_line.png\", dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "def plot_velocity_while_moving_normalized_line(df, folder, smoothing_n, global_nc):\n",
    "    plt.figure(figsize=(10,6))\n",
    "    for aid, grp in df.groupby('animal_id'):\n",
    "        g = grp.copy(); g['dnum'] = g['day_id'].str.extract(r'PD(\\d{2})').astype(int)\n",
    "        g = g.sort_values('dnum')\n",
    "        plt.plot(g['dnum'], g['mean_velocity_while_moving_pct-baseline'], marker='o', label=aid)\n",
    "    plt.axhline(0, linestyle='--', color='grey')\n",
    "    days = df[['day_id']].drop_duplicates(); days['dnum'] = days['day_id'].str.extract(r'PD(\\d{2})').astype(int)\n",
    "    days = days.sort_values('dnum')\n",
    "    plt.xticks(days['dnum'], days['day_id']); plt.ylim(0, None)\n",
    "    plt.xlabel('Days Post Stroke'); plt.ylabel('% Change Vel')\n",
    "    plt.title(f'Normalized Velocity Moving (gmm-{global_nc}, smooth-{smoothing_n})'); plt.legend(); plt.tight_layout()\n",
    "    plt.savefig(folder / f\"gmm-{global_nc}_smooth-{smoothing_n}_vel_moving_norm_line.png\", dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "def plot_velocity_nojumps_line(df, folder, smoothing_n, global_nc):\n",
    "    plt.figure(figsize=(10,6))\n",
    "    for aid, grp in df.groupby('animal_id'):\n",
    "        g = grp.copy(); g['dnum'] = g['day_id'].str.extract(r'PD(\\d{2})').astype(int)\n",
    "        g = g.sort_values('dnum')\n",
    "        plt.plot(g['dnum'], g['mean_velocity_while_moving_nojumps_cm_s'], marker='o', label=aid)\n",
    "    days = df[['day_id']].drop_duplicates(); days['dnum'] = days['day_id'].str.extract(r'PD(\\d{2})').astype(int)\n",
    "    days = days.sort_values('dnum')\n",
    "    plt.xticks(days['dnum'], days['day_id']); plt.ylim(0, None)\n",
    "    plt.xlabel('Days Post Stroke'); plt.ylabel('Mean Vel No Jumps (cm/s)')\n",
    "    plt.title(f'Velocity No Jumps (gmm-{global_nc}, smooth-{smoothing_n})'); plt.legend(); plt.tight_layout()\n",
    "    plt.savefig(folder / f\"gmm-{global_nc}_smooth-{smoothing_n}_vel_nojumps_line.png\", dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "def plot_velocity_nojumps_normalized_line(df, folder, smoothing_n, global_nc):\n",
    "    plt.figure(figsize=(10,6))\n",
    "    for aid, grp in df.groupby('animal_id'):\n",
    "        g = grp.copy(); g['dnum'] = g['day_id'].str.extract(r'PD(\\d{2})').astype(int)\n",
    "        g = g.sort_values('dnum')\n",
    "        plt.plot(g['dnum'], g['mean_velocity_while_moving_nojumps_pct-baseline'], marker='o', label=aid)\n",
    "    plt.axhline(0, linestyle='--', color='grey')\n",
    "    days = df[['day_id']].drop_duplicates(); days['dnum'] = days['day_id'].str.extract(r'PD(\\d{2})').astype(int)\n",
    "    days = days.sort_values('dnum')\n",
    "    plt.xticks(days['dnum'], days['day_id']); plt.ylim(0, None)\n",
    "    plt.xlabel('Days Post Stroke'); plt.ylabel('% Change Vel No Jumps')\n",
    "    plt.title(f'Normalized Vel No Jumps (gmm-{global_nc}, smooth-{smoothing_n})'); plt.legend(); plt.tight_layout()\n",
    "    plt.savefig(folder / f\"gmm-{global_nc}_smooth-{smoothing_n}_vel_nojumps_norm_line.png\", dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "# ------------------------------\n",
    "# PD00 Processing with Jump Removal\n",
    "# ------------------------------\n",
    "def process_pd00_file(file, smoothing_n):\n",
    "    df = pd.read_csv(file)\n",
    "    df.columns = df.columns.str.strip()\n",
    "    df = compute_velocity(df, smoothing_n)\n",
    "    aid, day = file.stem.split('_')\n",
    "    # remove jumps before GMM\n",
    "    for start, end in jumps.get((aid, day), []):\n",
    "        df = df[~df['frame'].between(start, end)]\n",
    "    valid = df['smoothed_velocity_px_s'].dropna().values.reshape(-1,1)\n",
    "    if len(valid) < 2:\n",
    "        return None\n",
    "    nc, gmm, bics = find_n_components_bic(valid)\n",
    "    w, m, c = gmm.weights_, gmm.means_.flatten(), gmm.covariances_.flatten()\n",
    "    idx = np.argsort(m)\n",
    "    thr = find_intersection(m[idx[:2]], c[idx[:2]], w[idx[:2]]) if nc>=2 else np.nan\n",
    "    return {\n",
    "        'base_name': file.stem,\n",
    "        'df': df,\n",
    "        'valid_velocities': valid,\n",
    "        'n_components': nc,\n",
    "        'gmm': gmm,\n",
    "        'bics': bics,\n",
    "        'threshold': thr\n",
    "    }\n",
    "\n",
    "def process_pd00_files(pd00_files, smoothing_n, plots_dir):\n",
    "    results = {}\n",
    "    for f in pd00_files:\n",
    "        res = process_pd00_file(f, smoothing_n)\n",
    "        if not res:\n",
    "            continue\n",
    "        results[res['base_name']] = res\n",
    "        plot_individual_histogram(res, plots_dir, smoothing_n)\n",
    "    return results\n",
    "\n",
    "def save_pd00_summary_metrics(pd00_res, out_root, smoothing_n, global_nc):\n",
    "    summary = []\n",
    "    for key, res in pd00_res.items():\n",
    "        aid, day = key.split('_')\n",
    "        summary.append({\n",
    "            'file': f\"{key}.csv\",\n",
    "            'animal_id': aid,\n",
    "            'day_id': day,\n",
    "            'n_components': res['n_components'],\n",
    "            'threshold': res['threshold']\n",
    "        })\n",
    "    if summary:\n",
    "        pd.DataFrame(summary).to_csv(\n",
    "            out_root / f\"gmm-{global_nc}_smooth-{smoothing_n}_PD00_summary_metrics.csv\",\n",
    "            index=False\n",
    "        )\n",
    "\n",
    "# ------------------------------\n",
    "# Main Analysis\n",
    "# ------------------------------\n",
    "def process_all_files(folder, smoothing_n, thresholds, time_dir):\n",
    "    pattern = re.compile(r\"^[A-Za-z]{2}\\d{3}_PD\\d{2}\\.csv$\")\n",
    "    files = [f for f in folder.iterdir() if f.is_file() and pattern.match(f.name)]\n",
    "    summary, base_pct, base_vel, base_vel_noj = [], {}, {}, {}\n",
    "\n",
    "    for f in files:\n",
    "        aid, day = f.stem.split('_')\n",
    "        if aid not in thresholds:\n",
    "            continue\n",
    "        df = pd.read_csv(f); df.columns = df.columns.str.strip()\n",
    "        df = compute_velocity(df, smoothing_n)\n",
    "        df['is_removed_jump'] = False\n",
    "        for start, end in jumps.get((aid, day), []):\n",
    "            df.loc[df['frame'].between(start, end), 'is_removed_jump'] = True\n",
    "        thr = thresholds[aid]['threshold']\n",
    "        df['is_moving'] = df['smoothed_velocity_px_s'] > thr\n",
    "\n",
    "        pct = df['is_moving'].mean() * 100\n",
    "        mv = df.loc[df['is_moving'], 'smoothed_velocity_px_s']\n",
    "        mv_cm = mv.mean()/PX_PER_CM if not mv.empty else np.nan\n",
    "        mv_noj = df.loc[df['is_moving'] & ~df['is_removed_jump'], 'smoothed_velocity_px_s']\n",
    "        mv_noj_cm = mv_noj.mean()/PX_PER_CM if not mv_noj.empty else np.nan\n",
    "\n",
    "        if day == \"PD00\":\n",
    "            base_pct[aid] = pct\n",
    "            base_vel[aid] = mv_cm\n",
    "            base_vel_noj[aid] = mv_noj_cm\n",
    "\n",
    "        df.to_csv(\n",
    "            time_dir / f\"{f.stem}_gmm-{thresholds[aid]['n_components']}_smooth-{smoothing_n}.csv\",\n",
    "            index=False\n",
    "        )\n",
    "\n",
    "        summary.append({\n",
    "            'animal_id': aid,\n",
    "            'day_id': day,\n",
    "            'pct_time_moving': pct,\n",
    "            'pct_time_moving_pct-baseline': np.nan,\n",
    "            'mean_velocity_while_moving_cm_s': mv_cm,\n",
    "            'mean_velocity_while_moving_pct-baseline': np.nan,\n",
    "            'mean_velocity_while_moving_nojumps_cm_s': mv_noj_cm,\n",
    "            'mean_velocity_while_moving_nojumps_pct-baseline': np.nan,\n",
    "            'n_components': thresholds[aid]['n_components'],\n",
    "            'smoothing_n': smoothing_n\n",
    "        })\n",
    "\n",
    "    for e in summary:\n",
    "        p0 = base_pct.get(e['animal_id'], np.nan)\n",
    "        e['pct_time_moving_pct-baseline'] = (e['pct_time_moving']/p0*100) if p0 else np.nan\n",
    "        v0 = base_vel.get(e['animal_id'], np.nan)\n",
    "        e['mean_velocity_while_moving_pct-baseline'] = (e['mean_velocity_while_moving_cm_s']/v0*100) if v0 else np.nan\n",
    "        vnj0 = base_vel_noj.get(e['animal_id'], np.nan)\n",
    "        e['mean_velocity_while_moving_nojumps_pct-baseline'] = (e['mean_velocity_while_moving_nojumps_cm_s']/vnj0*100) if vnj0 else np.nan\n",
    "\n",
    "    df_sum = pd.DataFrame(summary)[[\n",
    "        'animal_id','day_id',\n",
    "        'pct_time_moving','pct_time_moving_pct-baseline',\n",
    "        'mean_velocity_while_moving_cm_s','mean_velocity_while_moving_pct-baseline',\n",
    "        'mean_velocity_while_moving_nojumps_cm_s','mean_velocity_while_moving_nojumps_pct-baseline',\n",
    "        'n_components','smoothing_n'\n",
    "    ]]\n",
    "    return df_sum\n",
    "\n",
    "def main(analysis_folder, smoothing_n=SMOOTHING_N):\n",
    "    folder = Path(analysis_folder)\n",
    "    out = folder / \"gmm_analysis_auto\"\n",
    "    ip = out / \"individual_plots\"\n",
    "    tp = out / \"time_spent_moving\"\n",
    "    ip.mkdir(parents=True, exist_ok=True)\n",
    "    tp.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # PD00 analysis\n",
    "    pd00 = [f for f in folder.iterdir() if f.name.endswith(\"_PD00.csv\")]\n",
    "    pd00_res = process_pd00_files(pd00, smoothing_n, ip)\n",
    "    if not pd00_res:\n",
    "        print(\"No valid PD00 files found.\"); return\n",
    "\n",
    "    # use first PD00's n_components for naming\n",
    "    global_nc = list(pd00_res.values())[0]['n_components']  # Note this only works if all PD00 files have the same n_components\n",
    "\n",
    "    save_pd00_summary_metrics(pd00_res, out, smoothing_n, global_nc)\n",
    "    plot_bic_scores(pd00_res, out, smoothing_n, global_nc)\n",
    "    plot_distribution_pdf(pd00_res, out, smoothing_n, global_nc)\n",
    "\n",
    "    thresholds = {\n",
    "        k.split('_')[0]: {'threshold': v['threshold'], 'n_components': v['n_components']}\n",
    "        for k,v in pd00_res.items()\n",
    "    }\n",
    "\n",
    "    summary_df = process_all_files(folder, smoothing_n, thresholds, tp)\n",
    "\n",
    "    # summary file\n",
    "    summary_df.to_csv(\n",
    "        tp / f\"gmm-{global_nc}_smooth-{smoothing_n}_time_spent_moving_summary.csv\",\n",
    "        index=False\n",
    "    )\n",
    "\n",
    "    # plots\n",
    "    plot_time_spent_line(summary_df, tp, smoothing_n, global_nc)\n",
    "    plot_normalized_time_spent_line(summary_df, tp, smoothing_n, global_nc)\n",
    "    plot_velocity_while_moving_line(summary_df, tp, smoothing_n, global_nc)\n",
    "    plot_velocity_while_moving_normalized_line(summary_df, tp, smoothing_n, global_nc)\n",
    "    plot_velocity_nojumps_line(summary_df, tp, smoothing_n, global_nc)\n",
    "    plot_velocity_nojumps_normalized_line(summary_df, tp, smoothing_n, global_nc)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(ANALYSIS_FOLDER, SMOOTHING_N)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create videos with moving/still overlays based on GMM velocity thresholds\n",
    "\"\"\"\n",
    "Overlays a color-coded \"moving\" or \"still\" status on each video frame using thresholds\n",
    "derived from a GMM or similar baseline approach.\n",
    "\n",
    "1. Extracts a velocity threshold (or thresholds) from a summary metrics CSV.\n",
    "2. For each per-frame CSV in `csv_folder` (matching '*_processed.csv'):\n",
    "    - Computes smoothed velocity (rolling average) from centroid displacements.\n",
    "    - Finds the corresponding video (named '{animal_id}_{day_id}-trim_crop.mp4').\n",
    "    - For each frame, compares velocity vs. threshold, overlaying \"PROGRESS\" (moving)\n",
    "    or \"LINGER\" (still).\n",
    "3. Saves a new annotated video to an output directory named according to:\n",
    "    the threshold type (\"baseline\" or \"mean\") plus the GMM parameters.\n",
    "\n",
    "Args:\n",
    "    videos_folder (str or Path):\n",
    "        Directory containing the raw/truncated videos (named '{animal_id}_{day_id}-trim_crop.mp4').\n",
    "    csv_folder (str or Path):\n",
    "        Directory containing '*_processed.csv' files with centroid/time columns.\n",
    "    threshold_csv_path (str or Path):\n",
    "        Path to the summary metrics CSV that includes a 'threshold' column.\n",
    "        Expected filename format: 'method-method_n_smooth-smoothing_n_summary_metrics.csv'.\n",
    "    frame_width (int):\n",
    "        Desired output video width in pixels.\n",
    "    frame_height (int):\n",
    "        Desired output video height in pixels.\n",
    "    threshold_type (str):\n",
    "        Either 'baseline' or 'mean'.\n",
    "        - 'baseline': Use each animal's PD00 threshold.\n",
    "        - 'mean': Use the mean PD00 threshold across all animals.\n",
    "\n",
    "Notes:\n",
    "    - If video length > CSV rows, extra frames default to zero velocity.\n",
    "    - If CSV rows > video length, the extra rows go unused.\n",
    "    - Filenames for both videos and CSVs must follow a consistent pattern to match\n",
    "    animals and days (e.g., '{animal_id}_{day_id}-trim_crop.mp4' and\n",
    "    '{animal_id}_{day_id}_processed.csv').\n",
    "    - In 'baseline' mode, animals without PD00 entries in `threshold_csv_path` are skipped.\n",
    "\"\"\"\n",
    "\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "def extract_metadata_from_filename(path):\n",
    "    \"\"\"\n",
    "    Extracts metadata from a threshold CSV filename.\n",
    "    \n",
    "    Expected filename format: method-method_n_smooth-smoothing_n_summary_metrics.csv\n",
    "    Example: \"gmm-2_smooth-1_summary_metrics.csv\"\n",
    "    \n",
    "    Returns:\n",
    "      - method: The base method name (e.g., \"gmm\")\n",
    "      - method_n: The method version/number (e.g., \"2\")\n",
    "      - smoothing_n: The smoothing factor as an integer (e.g., 1)\n",
    "    \"\"\"\n",
    "    filename = Path(path).stem\n",
    "    match = re.match(r\"([^-]+)-(\\d+)_smooth-(\\d+)_summary_metrics\", filename)\n",
    "    if not match:\n",
    "        raise ValueError(\"Filename must follow format: method-method_n_smooth-smoothing_n_summary_metrics\")\n",
    "    method, method_n, smoothing_n = match.groups()\n",
    "    return method, method_n, int(smoothing_n)\n",
    "\n",
    "def process_videos(videos_folder, csv_folder, threshold_csv_path, frame_width, frame_height, threshold_type):\n",
    "    \"\"\"\n",
    "    Process videos to display moving/still status based on either animal-specific PD00 thresholds (baseline)\n",
    "    or the mean PD00 threshold (mean).\n",
    "    \"\"\"\n",
    "    method, method_n, smoothing_n = extract_metadata_from_filename(threshold_csv_path)\n",
    "    print(f\"Using threshold method: {method}-{method_n}\")\n",
    "    print(f\"Smoothing factor: {smoothing_n}\")\n",
    "\n",
    "    df_thresh = pd.read_csv(threshold_csv_path)\n",
    "\n",
    "    if threshold_type == \"baseline\":\n",
    "        df_thresh = df_thresh[df_thresh[\"day_id\"] == \"PD00\"]\n",
    "        animal_thresholds = dict(zip(df_thresh[\"animal_id\"], df_thresh[\"threshold\"]))\n",
    "        output_dir = Path(csv_folder) / f\"moving_threshold_videos_{method}-{method_n}_smooth-{smoothing_n}_thresh-Baseline\"\n",
    "    elif threshold_type == \"mean\":\n",
    "        avg_thresh = df_thresh[df_thresh[\"day_id\"] == \"PD00\"][\"threshold\"].mean()\n",
    "        animal_thresholds = None\n",
    "        output_dir = Path(csv_folder) / f\"moving_threshold_videos_{method}-{method_n}_smooth-{smoothing_n}_thresh-{avg_thresh:.2f}\"\n",
    "    else:\n",
    "        raise ValueError(\"threshold_type must be either 'baseline' or 'mean'\")\n",
    "\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for csv_file in Path(csv_folder).glob(\"*_processed.csv\"):\n",
    "        match = re.match(r\"([A-Za-z0-9]+)_([A-Za-z0-9]+)_processed\\.csv\", csv_file.name)\n",
    "        if not match:\n",
    "            continue\n",
    "        animal_id, day_id = match.groups()\n",
    "\n",
    "        if threshold_type == \"baseline\":\n",
    "            if animal_id not in animal_thresholds:\n",
    "                print(f\"Missing PD00 threshold for {animal_id}, skipping.\")\n",
    "                continue\n",
    "            thresh = animal_thresholds[animal_id]\n",
    "        else:\n",
    "            thresh = avg_thresh\n",
    "\n",
    "        df = pd.read_csv(csv_file)\n",
    "\n",
    "        if not {'centroid_x', 'centroid_y', 'real_time_s'}.issubset(df.columns):\n",
    "            print(f\"Missing centroid/time columns in {csv_file.name}, skipping.\")\n",
    "            continue\n",
    "\n",
    "        dx = np.diff(df[\"centroid_x\"])\n",
    "        dy = np.diff(df[\"centroid_y\"])\n",
    "        dt = np.diff(df[\"real_time_s\"])\n",
    "        velocity = np.sqrt(dx**2 + dy**2) / dt\n",
    "        df[\"velocity_px_s\"] = np.nan\n",
    "        df.loc[1:, \"velocity_px_s\"] = velocity\n",
    "        df[\"velocity_px_s\"] = df[\"velocity_px_s\"].rolling(window=smoothing_n, min_periods=1).mean()\n",
    "\n",
    "        video_name = f\"{animal_id}_{day_id}-trim_crop.mp4\"\n",
    "        video_path = Path(videos_folder) / video_name\n",
    "        if not video_path.exists():\n",
    "            print(f\"Video not found: {video_path}\")\n",
    "            continue\n",
    "\n",
    "        cap = cv2.VideoCapture(str(video_path))\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "        if threshold_type == \"baseline\":\n",
    "            out_name = f\"{animal_id}_{day_id}-moving_{method}-{method_n}_thresh-Baseline_smooth-{smoothing_n}.mp4\"\n",
    "        else:\n",
    "            out_name = f\"{animal_id}_{day_id}-moving_{method}-{method_n}_thresh-{avg_thresh:.2f}_smooth-{smoothing_n}.mp4\"\n",
    "\n",
    "        out_path = output_dir / out_name\n",
    "        fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "        out = cv2.VideoWriter(str(out_path), fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "        frame_idx = 0\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        font_scale = 1\n",
    "        thickness = 2\n",
    "\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            frame = cv2.resize(frame, (frame_width, frame_height))\n",
    "            velocity_val = df.loc[frame_idx, \"velocity_px_s\"] if frame_idx < len(df) else 0\n",
    "\n",
    "            if pd.isna(velocity_val) or velocity_val < thresh:\n",
    "                status_text, color = \"LINGER\", (0, 0, 255)\n",
    "            else:\n",
    "                status_text, color = \"PROGRESS\", (0, 255, 0)\n",
    "\n",
    "            text_size, _ = cv2.getTextSize(status_text, font, font_scale, thickness)\n",
    "            text_width, text_height = text_size\n",
    "            text_x = (frame_width - text_width) // 2\n",
    "            text_y = (frame_height + text_height) // 2\n",
    "\n",
    "            cv2.putText(frame, status_text, (text_x, text_y), font, font_scale, color, thickness)\n",
    "            out.write(frame)\n",
    "            frame_idx += 1\n",
    "\n",
    "        cap.release()\n",
    "        out.release()\n",
    "        print(f\"Processed: {out_path}\")\n",
    "\n",
    "VIDEOS_FOLDER = r\"D:\\CK3_open_field\\videos\"\n",
    "CSV_FOLDER = r\"D:\\CK3_open_field\\openfield_analysis_mask_2025-03-31\"\n",
    "THRESHOLD_CSV_PATH = r\"D:\\CK3_open_field\\openfield_analysis_mask_2025-03-31\\gmm_analysis_auto\\gmm-2_smooth-1_summary_metrics.csv\"\n",
    "FRAME_WIDTH = 700\n",
    "FRAME_HEIGHT = 700\n",
    "THRESHOLD_TYPE = \"baseline\"  # or \"mean\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_videos(\n",
    "        videos_folder=VIDEOS_FOLDER,\n",
    "        csv_folder=CSV_FOLDER,\n",
    "        threshold_csv_path=THRESHOLD_CSV_PATH,\n",
    "        frame_width=FRAME_WIDTH,\n",
    "        frame_height=FRAME_HEIGHT,\n",
    "        threshold_type=THRESHOLD_TYPE\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
